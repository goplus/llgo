package plan9asm

import (
	"fmt"
	"regexp"
	"sort"
	"strings"
)

type LLVMType string

const (
	Void LLVMType = "void"
	I1   LLVMType = "i1"
	I8   LLVMType = "i8"
	I16  LLVMType = "i16"
	I32  LLVMType = "i32"
	I64  LLVMType = "i64"
	Ptr  LLVMType = "ptr"
)

type FuncSig struct {
	Name  string
	Args  []LLVMType
	Ret   LLVMType // use Void for void-return
	Attrs string   // optional function attributes group (e.g. "#0")

	// ArgRegs optionally specifies which architectural registers correspond to
	// Args for arm64 asm that passes values in non-sequential registers.
	//
	// When empty, args are assumed to map to R0..R7 in order (ABIInternal-ish).
	//
	// This is used for intra-asm tailcalls like:
	//   B helper<>(SB)
	// where helper expects inputs in a custom register assignment.
	ArgRegs []Reg

	// Frame provides a minimal stack-frame model for resolving name+off(FP)
	// references in Go/Plan9 assembly into LLVM function args/returns.
	//
	// This is intentionally limited to the classic Go assembler convention used
	// by stdlib .s files (e.g. internal/cpu/cpu_x86.s).
	Frame FrameLayout
}

type FrameLayout struct {
	Params  []FrameSlot
	Results []FrameSlot
}

type FrameSlot struct {
	Offset int64
	Type   LLVMType
	Index  int // index into LLVM function arguments (for Params) or results tuple (for Results)
	// Field is the index of the extracted field within the argument aggregate.
	// It is used for classic Go asm slots like b_base+0(FP) when the Go-level
	// parameter is passed as a struct (string/slice header).
	//
	// When Field < 0, the slot refers directly to %arg(Index).
	Field int
}

type Options struct {
	TargetTriple string

	// ResolveSym maps the TEXT symbol (with (SB) trimmed) into the final linker
	// symbol name to emit in LLVM IR. If nil, the symbol is used as-is.
	ResolveSym func(sym string) string

	// Sigs maps resolved symbol name -> signature.
	Sigs map[string]FuncSig

	// Goarch is used for a few arch-specific translations (e.g. x86 CPUID).
	Goarch string

	// AnnotateSource emits source asm lines as IR comments before lowering each
	// instruction, for translation debugging.
	AnnotateSource bool
}

// Translate converts a parsed Plan 9 asm File into LLVM IR text (`.ll`).
//
// This is intentionally a prototype and supports only a small subset.
func Translate(file *File, opt Options) (string, error) {
	if file == nil {
		return "", fmt.Errorf("nil file")
	}
	if len(file.Funcs) == 0 {
		return "", fmt.Errorf("empty file")
	}

	resolve := opt.ResolveSym
	if resolve == nil {
		resolve = func(s string) string { return s }
	}

	var b strings.Builder
	b.WriteString("; Generated by llgo internal/plan9asm (prototype)\n")
	if opt.TargetTriple != "" {
		fmt.Fprintf(&b, "target triple = %q\n\n", opt.TargetTriple)
	}
	// Intrinsics used by the prototype lowering. Declare them up-front so clang/llc
	// can type-check the module without relying on implicit declarations.
	if file.Arch == ArchARM64 {
		b.WriteString("declare i64 @syscall(i64, i64, i64, i64, i64, i64, i64)\n")
		b.WriteString("declare i32 @cliteErrno()\n")
		b.WriteString("declare i64 @llvm.bitreverse.i64(i64)\n")
		b.WriteString("declare i64 @llvm.ctlz.i64(i64, i1)\n")
		b.WriteString("declare i64 @llvm.bswap.i64(i64)\n")
		// AArch64 CRC32 and CRC32C intrinsics.
		// Note: B/H forms take the data operand as i32 (low bits used).
		b.WriteString("declare i32 @llvm.aarch64.crc32b(i32, i32)\n")
		b.WriteString("declare i32 @llvm.aarch64.crc32h(i32, i32)\n")
		b.WriteString("declare i32 @llvm.aarch64.crc32w(i32, i32)\n")
		b.WriteString("declare i32 @llvm.aarch64.crc32x(i32, i64)\n")
		b.WriteString("declare i32 @llvm.aarch64.crc32cb(i32, i32)\n")
		b.WriteString("declare i32 @llvm.aarch64.crc32ch(i32, i32)\n")
		b.WriteString("declare i32 @llvm.aarch64.crc32cw(i32, i32)\n")
		b.WriteString("declare i32 @llvm.aarch64.crc32cx(i32, i64)\n")
		b.WriteString("\n")
		// Attribute group used by some functions to enable optional ISA features.
		// (Example: "+crc" for hash/crc32 arm64 fast paths.)
		b.WriteString("attributes #0 = { \"target-features\"=\"+crc\" }\n\n")
	}
	if file.Arch == ArchAMD64 && opt.Goarch == "amd64" {
		b.WriteString("declare i64 @syscall(i64, i64, i64, i64, i64, i64, i64)\n")
		b.WriteString("declare i32 @cliteErrno()\n")
		// Generic LLVM intrinsics used by amd64 lowering.
		b.WriteString("declare i64 @llvm.cttz.i64(i64, i1)\n")
		b.WriteString("declare i32 @llvm.cttz.i32(i32, i1)\n")
		b.WriteString("declare i64 @llvm.ctlz.i64(i64, i1)\n")
		b.WriteString("declare i32 @llvm.ctlz.i32(i32, i1)\n")
		b.WriteString("declare i64 @llvm.ctpop.i64(i64)\n")
		b.WriteString("declare i32 @llvm.ctpop.i32(i32)\n")
		b.WriteString("declare i64 @llvm.bswap.i64(i64)\n")
		b.WriteString("declare double @llvm.sqrt.f64(double)\n")
		b.WriteString("declare double @llvm.rint.f64(double)\n")
		b.WriteString("\n")
		// x86-64 CRC32 (SSE4.2) and PCLMULQDQ intrinsics.
		b.WriteString("declare i64 @llvm.x86.sse42.crc32.64.64(i64, i64)\n")
		b.WriteString("declare i32 @llvm.x86.sse42.crc32.32.32(i32, i32)\n")
		b.WriteString("declare i32 @llvm.x86.sse42.crc32.32.16(i32, i16)\n")
		b.WriteString("declare i32 @llvm.x86.sse42.crc32.32.8(i32, i8)\n")
		b.WriteString("declare <2 x i64> @llvm.x86.pclmulqdq(<2 x i64>, <2 x i64>, i8 immarg)\n")
		// SSE2 helpers used by stdlib asm (e.g. internal/bytealg).
		b.WriteString("declare i32 @llvm.x86.sse2.pmovmskb.128(<16 x i8>)\n")
		b.WriteString("\n")
		// Attribute groups for optional ISA features:
		// - #0: SSE4.2 CRC32 instruction (Castagnoli fast path)
		// - #1: PCLMULQDQ + SSE4.1 (IEEE fast path)
		b.WriteString("attributes #0 = { \"target-features\"=\"+sse4.2,+crc32\" }\n")
		b.WriteString("attributes #1 = { \"target-features\"=\"+pclmul,+sse4.1\" }\n\n")
	}

	emitExternSBGlobals(&b, file, resolve)

	if len(file.Data) != 0 || len(file.Globl) != 0 {
		if err := emitDataGlobals(&b, file, resolve); err != nil {
			return "", err
		}
		b.WriteString("\n")
	}

	emitExternFuncDecls(&b, file, resolve, opt.Sigs)

	for i := range file.Funcs {
		fn := &file.Funcs[i]
		name := resolve(fn.Sym)
		sig, ok := opt.Sigs[name]
		if !ok {
			return "", fmt.Errorf("missing signature for %q", name)
		}
		if sig.Name == "" {
			sig.Name = name
		}
		if sig.Name != name {
			return "", fmt.Errorf("signature name mismatch: %q vs %q", sig.Name, name)
		}
		if sig.Ret == "" {
			return "", fmt.Errorf("missing return type for %q", name)
		}
		if file.Arch == ArchARM64 && funcNeedsARM64CFG(*fn) {
			if err := translateFuncARM64(&b, *fn, sig, resolve, opt.Sigs, opt.AnnotateSource); err != nil {
				return "", fmt.Errorf("%s: %v", name, err)
			}
			b.WriteString("\n")
			continue
		}
		if file.Arch == ArchAMD64 && opt.Goarch == "amd64" && funcNeedsAMD64CFG(*fn) {
			if err := translateFuncAMD64(&b, *fn, sig, resolve, opt.Sigs, opt.AnnotateSource); err != nil {
				return "", fmt.Errorf("%s: %v", name, err)
			}
			b.WriteString("\n")
			continue
		}
		if err := translateFuncLinear(&b, file.Arch, *fn, sig, opt.AnnotateSource); err != nil {
			return "", fmt.Errorf("%s: %v", name, err)
		}
		b.WriteString("\n")
	}
	return b.String(), nil
}

func emitExternFuncDecls(b *strings.Builder, file *File, resolve func(string) string, sigs map[string]FuncSig) {
	defined := map[string]bool{}
	for i := range file.Funcs {
		defined[resolve(file.Funcs[i].Sym)] = true
	}

	// Deterministic order for tests/debugging.
	names := make([]string, 0, len(sigs))
	for name := range sigs {
		if defined[name] {
			continue
		}
		names = append(names, name)
	}
	sort.Strings(names)

	for _, name := range names {
		sig := sigs[name]
		if sig.Ret == "" {
			continue
		}
		fmt.Fprintf(b, "declare %s %s(", sig.Ret, llvmGlobal(name))
		for i, t := range sig.Args {
			if i > 0 {
				b.WriteString(", ")
			}
			b.WriteString(string(t))
		}
		b.WriteString(")")
		if sig.Attrs != "" {
			b.WriteString(" " + sig.Attrs)
		}
		b.WriteString("\n")
	}
	if len(names) != 0 {
		b.WriteString("\n")
	}
}

func emitExternSBGlobals(b *strings.Builder, file *File, resolve func(string) string) {
	// Collect base symbols of SB refs with numeric offsets. These are almost always
	// global variables whose address is computed via GEP in the lowering.
	need := map[string]bool{}
	defined := map[string]bool{}

	// Anything in DATA/GLOBL will be defined in this module.
	for _, g := range file.Globl {
		defined[resolve(g.Sym)] = true
	}
	for _, d := range file.Data {
		defined[resolve(d.Sym)] = true
	}

	for _, fn := range file.Funcs {
		for _, ins := range fn.Instrs {
			opName := strings.ToUpper(string(ins.Op))
			for _, arg := range ins.Args {
				if arg.Kind != OpSym {
					continue
				}
				s := strings.TrimSpace(arg.Sym)
				if !strings.HasSuffix(s, "(SB)") {
					continue
				}
				s = strings.TrimSuffix(s, "(SB)")
				base, off := splitSymPlusOff(s)
				if base == "" {
					continue
				}
				if off == 0 {
					// Bare symbol refs are usually global data addresses
					// (e.g. MOVQ runtime·vdsoGettimeofdaySym(SB), AX). Exclude only
					// control-flow ops that use symbol operands as branch/call targets.
					switch opName {
					case "JMP", "JE", "JEQ", "JZ", "JNE", "JNZ",
						"JL", "JLT", "JLE", "JG", "JGT", "JGE",
						"JB", "JBE", "JA", "JAE", "JLS",
						"JC", "JNC", "JCC", "CALL", "BL", "B":
						continue
					}
				}
				name := resolve(base)
				if name != "" && !defined[name] {
					need[name] = true
				}
			}
		}
	}

	if len(need) == 0 {
		return
	}
	for name := range need {
		fmt.Fprintf(b, "%s = external global i8\n", llvmGlobal(name))
	}
	b.WriteString("\n")
}

func emitDataGlobals(b *strings.Builder, file *File, resolve func(string) string) error {
	// Merge DATA and GLOBL into resolved symbol -> bytes.
	type symData struct {
		size  int64
		bytes map[int64][]byte // off -> payload
	}

	syms := map[string]*symData{}

	resolveData := func(sym string) string {
		// Heuristic: unqualified plain names in a stdlib .s are package-local.
		// Reuse ResolveSym's local-name behavior by prefixing a Plan9 middle dot.
		if strings.Contains(sym, "·") || strings.Contains(sym, "/") || strings.Contains(sym, ".") {
			return resolve(sym)
		}
		return resolve("·" + sym)
	}

	for _, g := range file.Globl {
		name := resolveData(g.Sym)
		sd := syms[name]
		if sd == nil {
			sd = &symData{bytes: map[int64][]byte{}}
			syms[name] = sd
		}
		if g.Size > sd.size {
			sd.size = g.Size
		}
	}

	for _, d := range file.Data {
		name := resolveData(d.Sym)
		sd := syms[name]
		if sd == nil {
			sd = &symData{bytes: map[int64][]byte{}}
			syms[name] = sd
		}
		if d.Width <= 0 {
			return fmt.Errorf("DATA %s: invalid width %d", d.Sym, d.Width)
		}
		payload := make([]byte, d.Width)
		// Plan 9 asm DATA encodes immediates little-endian on amd64/arm64.
		v := d.Value
		for i := int64(0); i < d.Width; i++ {
			payload[i] = byte(v & 0xff)
			v >>= 8
		}
		sd.bytes[d.Off] = payload
		if end := d.Off + d.Width; end > sd.size {
			sd.size = end
		}
	}

	if len(syms) == 0 {
		return nil
	}

	// Deterministic output order.
	names := make([]string, 0, len(syms))
	for n := range syms {
		names = append(names, n)
	}
	sort.Strings(names)

	for _, name := range names {
		sd := syms[name]
		if sd.size <= 0 {
			continue
		}
		buf := make([]byte, sd.size)
		for off, p := range sd.bytes {
			if off < 0 || off+int64(len(p)) > int64(len(buf)) {
				return fmt.Errorf("DATA %s: out of bounds off=%d len=%d size=%d", name, off, len(p), len(buf))
			}
			copy(buf[off:], p)
		}
		align := bestAlign(int64(len(buf)))
		fmt.Fprintf(b, "%s = constant [%d x i8] %s, align %d\n", llvmGlobal(name), len(buf), llvmI8ArrayInit(buf), align)
	}
	return nil
}

func bestAlign(size int64) int64 {
	// Conservative alignment guess good enough for stdlib constant tables.
	switch {
	case size >= 16 && size%16 == 0:
		return 16
	case size >= 8 && size%8 == 0:
		return 8
	case size >= 4 && size%4 == 0:
		return 4
	case size >= 2 && size%2 == 0:
		return 2
	default:
		return 1
	}
}

func llvmI8ArrayInit(b []byte) string {
	if len(b) == 0 {
		return "zeroinitializer"
	}
	var sb strings.Builder
	sb.WriteString("[")
	for i, v := range b {
		if i != 0 {
			sb.WriteString(", ")
		}
		fmt.Fprintf(&sb, "i8 %d", int(v))
	}
	sb.WriteString("]")
	return sb.String()
}

func translateFuncLinear(b *strings.Builder, arch Arch, fn Func, sig FuncSig, annotateSource bool) error {
	// Function header.
	fmt.Fprintf(b, "define %s %s(", sig.Ret, llvmGlobal(sig.Name))
	for i, t := range sig.Args {
		if i > 0 {
			b.WriteString(", ")
		}
		fmt.Fprintf(b, "%s %%arg%d", t, i)
	}
	b.WriteString(")")
	if sig.Attrs != "" {
		b.WriteString(" " + sig.Attrs)
	}
	b.WriteString(" {\n")
	b.WriteString("entry:\n")

	// Fast path: void marker functions (TEXT ...; RET; and optionally BYTE).
	if sig.Ret == Void {
		for _, ins := range fn.Instrs {
			switch ins.Op {
			case OpTEXT, OpRET, OpBYTE:
				// ignore
			default:
				return fmt.Errorf("unsupported opcode in void function: %s", ins.Op)
			}
		}
		b.WriteString("  ret void\n")
		b.WriteString("}\n")
		return nil
	}

	type ssaVal struct {
		typ LLVMType
		val string // either constant ("0") or SSA ("%t1")
	}

	isSSA := func(v string) bool { return strings.HasPrefix(v, "%") }

	// Register SSA values.
	reg := map[Reg]ssaVal{}
	results := make([]ssaVal, len(sig.Frame.Results))
	haveResult := make([]bool, len(sig.Frame.Results))

	// Initialize a few common arg registers for ABIInternal-style asm.
	// This is currently best-effort; the prototype primarily targets stdlib
	// asm that uses FP slots.
	switch arch {
	case ArchARM64:
		if len(sig.ArgRegs) > 0 {
			for i := 0; i < len(sig.Args) && i < len(sig.ArgRegs); i++ {
				reg[sig.ArgRegs[i]] = ssaVal{typ: sig.Args[i], val: fmt.Sprintf("%%arg%d", i)}
			}
		} else {
			for i := 0; i < len(sig.Args) && i < 8; i++ {
				reg[Reg(fmt.Sprintf("R%d", i))] = ssaVal{typ: sig.Args[i], val: fmt.Sprintf("%%arg%d", i)}
			}
		}
	case ArchAMD64:
		if len(sig.ArgRegs) > 0 {
			for i := 0; i < len(sig.Args) && i < len(sig.ArgRegs); i++ {
				reg[sig.ArgRegs[i]] = ssaVal{typ: sig.Args[i], val: fmt.Sprintf("%%arg%d", i)}
			}
		} else {
			// SysV-ish mapping (C ABI): DI, SI, DX, CX, R8, R9.
			x86 := []Reg{DI, SI, DX, CX, Reg("R8"), Reg("R9")}
			for i := 0; i < len(sig.Args) && i < len(x86); i++ {
				reg[x86[i]] = ssaVal{typ: sig.Args[i], val: fmt.Sprintf("%%arg%d", i)}
			}
		}
	}
	tmp := 0
	newTmp := func() string {
		tmp++
		return fmt.Sprintf("t%d", tmp)
	}

	emitCast := func(v ssaVal, to LLVMType) (ssaVal, error) {
		if v.typ == "" {
			return ssaVal{}, fmt.Errorf("missing type for value %q", v.val)
		}
		if v.typ == to {
			return v, nil
		}
		// Only support integer casts for now (enough for internal/cpu asm).
		switch {
		case v.typ == I64 && to == I32:
			name := newTmp()
			fmt.Fprintf(b, "  %%%s = trunc i64 %s to i32\n", name, v.val)
			return ssaVal{typ: I32, val: "%" + name}, nil
		case v.typ == I32 && to == I64:
			name := newTmp()
			fmt.Fprintf(b, "  %%%s = zext i32 %s to i64\n", name, v.val)
			return ssaVal{typ: I64, val: "%" + name}, nil
		case v.typ == Ptr && to == I64:
			// stdlib asm often moves pointers through GPRs (e.g. MOVD ptr+0(FP), R0).
			// Linear lowering models GPRs as integer SSA values, so support ptr<->i64.
			if !isSSA(v.val) {
				if v.val == "0" {
					return ssaVal{typ: I64, val: "0"}, nil
				}
				return ssaVal{}, fmt.Errorf("unsupported non-SSA ptr cast source %q", v.val)
			}
			name := newTmp()
			fmt.Fprintf(b, "  %%%s = ptrtoint ptr %s to i64\n", name, v.val)
			return ssaVal{typ: I64, val: "%" + name}, nil
		case v.typ == I64 && to == Ptr:
			src := v.val
			if !isSSA(src) {
				name := newTmp()
				fmt.Fprintf(b, "  %%%s = add i64 %s, 0\n", name, src)
				src = "%" + name
			}
			name := newTmp()
			fmt.Fprintf(b, "  %%%s = inttoptr i64 %s to ptr\n", name, src)
			return ssaVal{typ: Ptr, val: "%" + name}, nil
		default:
			return ssaVal{}, fmt.Errorf("unsupported cast %s -> %s", v.typ, to)
		}
	}

	zero := func(t LLVMType) (ssaVal, error) {
		switch t {
		case I32, I64:
			return ssaVal{typ: t, val: "0"}, nil
		default:
			return ssaVal{}, fmt.Errorf("zero: unsupported type %q", t)
		}
	}

	fpParamSlot := func(off int64) (FrameSlot, bool) {
		for _, s := range sig.Frame.Params {
			if s.Offset == off {
				return s, true
			}
		}
		return FrameSlot{}, false
	}
	fpResultIndex := func(off int64) (int, LLVMType, bool) {
		for _, s := range sig.Frame.Results {
			if s.Offset == off {
				return s.Index, s.Type, true
			}
		}
		return 0, "", false
	}

	valueOf := func(op Operand) (ssaVal, error) {
		switch op.Kind {
		case OpImm:
			// Default immediates to i64; MOVL will cast to i32 as needed.
			return ssaVal{typ: I64, val: fmt.Sprintf("%d", op.Imm)}, nil
		case OpReg:
			v, ok := reg[op.Reg]
			if !ok {
				// Uninitialized register -> treat as 0 i64 for now.
				return ssaVal{typ: I64, val: "0"}, nil
			}
			return v, nil
		case OpFP:
			slot, ok := fpParamSlot(op.FPOffset)
			if ok {
				idx := slot.Index
				if idx < 0 || idx >= len(sig.Args) {
					return ssaVal{}, fmt.Errorf("FP slot %s invalid arg index %d", op.String(), idx)
				}
				arg := fmt.Sprintf("%%arg%d", idx)
				if slot.Field >= 0 {
					aggTy := sig.Args[idx]
					name := newTmp()
					fmt.Fprintf(b, "  %%%s = extractvalue %s %s, %d\n", name, aggTy, arg, slot.Field)
					return ssaVal{typ: slot.Type, val: "%" + name}, nil
				}
				return ssaVal{typ: slot.Type, val: arg}, nil
			}
			return ssaVal{}, fmt.Errorf("unsupported FP read slot: %s", op.String())
		default:
			return ssaVal{}, fmt.Errorf("invalid operand: %v", op)
		}
	}

	setReg := func(r Reg, v ssaVal) error {
		if v.typ == "" {
			return fmt.Errorf("setReg(%s): missing type", r)
		}
		// Materialize constants into SSA to simplify later inline asm.
		if !isSSA(v.val) {
			name := newTmp()
			fmt.Fprintf(b, "  %%%s = add %s %s, 0\n", name, v.typ, v.val)
			v.val = "%" + name
		}
		reg[r] = v
		return nil
	}

	setResult := func(off int64, v ssaVal) error {
		idx, ty, ok := fpResultIndex(off)
		if !ok {
			return fmt.Errorf("unsupported FP write slot: +%d(FP)", off)
		}
		if v.typ != ty {
			var err error
			v, err = emitCast(v, ty)
			if err != nil {
				return err
			}
		}
		results[idx] = v
		haveResult[idx] = true
		return nil
	}

	for _, ins := range fn.Instrs {
		if annotateSource {
			emitIRSourceComment(b, ins.Raw)
		}
		switch ins.Op {
		case OpTEXT:
			continue
		case OpMRS:
			// ARM64: MRS <sysreg>, Rn
			src, dst := ins.Args[0], ins.Args[1]
			if src.Kind != OpIdent || dst.Kind != OpReg {
				return fmt.Errorf("MRS expects ident, reg: %q", ins.Raw)
			}
			name := newTmp()
			// Read system register via inline asm.
			// Example: call i64 asm "mrs $0, MIDR_EL1", "=r"()
			fmt.Fprintf(b, "  %%%s = call i64 asm %q, %q()\n", name, "mrs $0, "+src.Ident, "=r")
			reg[dst.Reg] = ssaVal{typ: I64, val: "%" + name}
			continue
		case OpMOVD:
			// ARM64: MOVD src, dst
			src, dst := ins.Args[0], ins.Args[1]
			v, err := valueOf(src)
			if err != nil {
				return err
			}
			v, err = emitCast(v, I64)
			if err != nil {
				return err
			}
			switch dst.Kind {
			case OpReg:
				if err := setReg(dst.Reg, v); err != nil {
					return err
				}
			case OpFP:
				if err := setResult(dst.FPOffset, v); err != nil {
					return err
				}
			default:
				return fmt.Errorf("MOVD dst unsupported: %s", dst.String())
			}
			continue
		case OpMOVQ:
			src, dst := ins.Args[0], ins.Args[1]
			v, err := valueOf(src)
			if err != nil {
				return err
			}
			v, err = emitCast(v, I64)
			if err != nil {
				return err
			}
			switch dst.Kind {
			case OpReg:
				if err := setReg(dst.Reg, v); err != nil {
					return err
				}
			case OpFP:
				if err := setResult(dst.FPOffset, v); err != nil {
					return err
				}
			default:
				return fmt.Errorf("MOVQ dst unsupported: %s", dst.String())
			}

		case OpADDQ, OpSUBQ, OpXORQ:
			src, dst := ins.Args[0], ins.Args[1]
			if dst.Kind != OpReg {
				return fmt.Errorf("%s dst must be register in prototype: %s", ins.Op, dst.String())
			}
			lhs, err := valueOf(dst)
			if err != nil {
				return err
			}
			rhs, err := valueOf(src)
			if err != nil {
				return err
			}
			lhs, err = emitCast(lhs, I64)
			if err != nil {
				return err
			}
			rhs, err = emitCast(rhs, I64)
			if err != nil {
				return err
			}
			name := newTmp()
			switch ins.Op {
			case OpADDQ:
				fmt.Fprintf(b, "  %%%s = add i64 %s, %s\n", name, lhs.val, rhs.val)
			case OpSUBQ:
				fmt.Fprintf(b, "  %%%s = sub i64 %s, %s\n", name, lhs.val, rhs.val)
			case OpXORQ:
				fmt.Fprintf(b, "  %%%s = xor i64 %s, %s\n", name, lhs.val, rhs.val)
			}
			reg[dst.Reg] = ssaVal{typ: I64, val: "%" + name}

		case OpMOVL:
			src, dst := ins.Args[0], ins.Args[1]
			v, err := valueOf(src)
			if err != nil {
				return err
			}
			v, err = emitCast(v, I32)
			if err != nil {
				return err
			}
			switch dst.Kind {
			case OpReg:
				if err := setReg(dst.Reg, v); err != nil {
					return err
				}
			case OpFP:
				if err := setResult(dst.FPOffset, v); err != nil {
					return err
				}
			default:
				return fmt.Errorf("MOVL dst unsupported: %s", dst.String())
			}

		case OpCPUID:
			// x86: CPUID reads EAX/ECX and writes EAX/EBX/ECX/EDX.
			eax := reg[AX]
			if eax.typ == "" {
				z, _ := zero(I32)
				eax = z
			} else {
				var err error
				eax, err = emitCast(eax, I32)
				if err != nil {
					return err
				}
			}
			ecx := reg[CX]
			if ecx.typ == "" {
				z, _ := zero(I32)
				ecx = z
			} else {
				var err error
				ecx, err = emitCast(ecx, I32)
				if err != nil {
					return err
				}
			}
			if !isSSA(eax.val) {
				if err := setReg(AX, eax); err != nil {
					return err
				}
				eax = reg[AX]
			}
			if !isSSA(ecx.val) {
				if err := setReg(CX, ecx); err != nil {
					return err
				}
				ecx = reg[CX]
			}
			call := newTmp()
			// Return 4x i32 in EAX/EBX/ECX/EDX.
			fmt.Fprintf(b, "  %%%s = call { i32, i32, i32, i32 } asm sideeffect %q, %q(i32 %s, i32 %s)\n",
				call,
				"cpuid",
				"={ax},={bx},={cx},={dx},{ax},{cx},~{dirflag},~{fpsr},~{flags}",
				eax.val, ecx.val)
			ext := func(i int) string {
				n := newTmp()
				fmt.Fprintf(b, "  %%%s = extractvalue { i32, i32, i32, i32 } %%%s, %d\n", n, call, i)
				return "%" + n
			}
			reg[AX] = ssaVal{typ: I32, val: ext(0)}
			reg[BX] = ssaVal{typ: I32, val: ext(1)}
			reg[CX] = ssaVal{typ: I32, val: ext(2)}
			reg[DX] = ssaVal{typ: I32, val: ext(3)}

		case OpXGETBV:
			// x86: XGETBV reads ECX and writes EAX/EDX.
			ecx := reg[CX]
			if ecx.typ == "" {
				z, _ := zero(I32)
				ecx = z
			} else {
				var err error
				ecx, err = emitCast(ecx, I32)
				if err != nil {
					return err
				}
			}
			if !isSSA(ecx.val) {
				if err := setReg(CX, ecx); err != nil {
					return err
				}
				ecx = reg[CX]
			}
			call := newTmp()
			fmt.Fprintf(b, "  %%%s = call { i32, i32 } asm sideeffect %q, %q(i32 %s)\n",
				call,
				"xgetbv",
				"={ax},={dx},{cx},~{dirflag},~{fpsr},~{flags}",
				ecx.val)
			eaxN := newTmp()
			edxN := newTmp()
			fmt.Fprintf(b, "  %%%s = extractvalue { i32, i32 } %%%s, 0\n", eaxN, call)
			fmt.Fprintf(b, "  %%%s = extractvalue { i32, i32 } %%%s, 1\n", edxN, call)
			reg[AX] = ssaVal{typ: I32, val: "%" + eaxN}
			reg[DX] = ssaVal{typ: I32, val: "%" + edxN}

		case OpRET:
			// Return value comes either from explicit result slots (name+off(FP))
			// or, as a fallback, from the arch return register for scalar returns.
			switch {
			case sig.Ret == Void:
				b.WriteString("  ret void\n")
			case len(sig.Frame.Results) > 1:
				// Aggregate return.
				cur := "undef"
				last := ""
				for _, slot := range sig.Frame.Results {
					i := slot.Index
					v := ssaVal{typ: slot.Type, val: "0"}
					if haveResult[i] {
						v = results[i]
					} else {
						z, err := zero(slot.Type)
						if err != nil {
							return err
						}
						v = z
					}
					if v.typ != slot.Type {
						var err error
						v, err = emitCast(v, slot.Type)
						if err != nil {
							return err
						}
					}
					if !isSSA(v.val) {
						// insertvalue accepts constants, but normalize to SSA for consistency.
						name := newTmp()
						fmt.Fprintf(b, "  %%%s = add %s %s, 0\n", name, v.typ, v.val)
						v.val = "%" + name
					}
					name := newTmp()
					fmt.Fprintf(b, "  %%%s = insertvalue %s %s, %s %s, %d\n", name, sig.Ret, cur, slot.Type, v.val, i)
					cur = "%" + name
					last = cur
				}
				fmt.Fprintf(b, "  ret %s %s\n", sig.Ret, last)
			default:
				// Scalar return.
				var v ssaVal
				if len(sig.Frame.Results) == 1 && haveResult[0] {
					v = results[0]
				} else if rv, ok := reg[archReturnReg(arch)]; ok {
					v = rv
				} else {
					z, err := zero(sig.Ret)
					if err != nil {
						return err
					}
					v = z
				}
				if v.typ != sig.Ret {
					var err error
					v, err = emitCast(v, sig.Ret)
					if err != nil {
						return err
					}
				}
				fmt.Fprintf(b, "  ret %s %s\n", sig.Ret, v.val)
			}

		case OpBYTE:
			// Ignore raw machine bytes for now (prototype).
			continue
		default:
			return fmt.Errorf("unsupported instruction: %s", ins.Op)
		}
	}

	b.WriteString("}\n")
	return nil
}

func archReturnReg(arch Arch) Reg {
	if arch == ArchARM64 {
		return Reg("R0")
	}
	return AX
}

var llvmIdentRe = regexp.MustCompile(`^[A-Za-z_][A-Za-z0-9_]*$`)

func llvmGlobal(name string) string {
	// LLVM requires quoting if name contains special characters (like / or .).
	if llvmIdentRe.MatchString(name) {
		return "@" + name
	}
	return "@\"" + strings.ReplaceAll(name, "\"", "\\\"") + "\""
}
